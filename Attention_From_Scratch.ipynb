{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f24a58b",
   "metadata": {},
   "source": [
    "\n",
    "# 🧠 Attention From Scratch: A 5‑Lesson Math‑First Mini‑Course (Python Only)\n",
    "\n",
    "**Audience:** Freshman → Senior undergrads in Math/CS (or motivated self‑learners)  \n",
    "**Goal:** Understand the math *under the hood* of modern AI (especially Transformers) by coding everything from first principles—no NumPy, no PyTorch.  \n",
    "**Format:** 5 lessons, each with *explanations, code, exercises,* and *print‑outs* so you see what's happening.\n",
    "\n",
    "### What you'll build\n",
    "1. **Linear Algebra Toolkit** (vectors, dot products, matrix multiply, transpose, cosine similarity)\n",
    "2. **Probability Toolkit** (softmax, cross‑entropy, batch operations)\n",
    "3. **Optimization Basics** (derivatives, numerical gradient, gradient descent)\n",
    "4. **Tiny Neural Net From Scratch** (solve XOR with manual backprop)\n",
    "5. **Scaled Dot‑Product Attention** (the core of Transformers), including **causal masking**\n",
    "\n",
    "> **Why no heavy libraries?** To remove the “black box” and make the math tangible. When you *write* `matmul`, `softmax`, or `attention` yourself, you *feel* why these operations matter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135d451c",
   "metadata": {},
   "source": [
    "\n",
    "## 🏁 Getting started\n",
    "\n",
    "- Run each cell in order (Shift+Enter).\n",
    "- Read the printed output. Tweak values and re‑run to build intuition.\n",
    "- Each lesson ends with **Try it** prompts so you can stretch further.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6121542c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Minimal dependencies: only Python stdlib\n",
    "import math, random\n",
    "random.seed(42)\n",
    "\n",
    "def banner(title):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(title)\n",
    "    print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd2a919",
   "metadata": {},
   "source": [
    "\n",
    "## 🔧 Core utilities we’ll reuse\n",
    "We keep these helpers tiny and explicit. If you don't understand a function, scroll back and study it—there's no magic here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5ada1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def zeros_like(A):\n",
    "    '''Return a zero-structure with the same shape as A (list or list of lists).'''\n",
    "    if isinstance(A, list) and len(A) > 0 and isinstance(A[0], list):\n",
    "        return [[0.0 for _ in row] for row in A]\n",
    "    return [0.0 for _ in A]\n",
    "\n",
    "def mat_shape(A):\n",
    "    '''Return shape of a vector or matrix stored as nested Python lists.'''\n",
    "    if isinstance(A[0], list): return (len(A), len(A[0]))\n",
    "    return (len(A), )\n",
    "\n",
    "def matmul(A, B):\n",
    "    '''\n",
    "    Matrix multiply A (m x n) by B (n x p) -> (m x p).\n",
    "    Implemented explicitly to show the triple loop.\n",
    "    '''\n",
    "    m, n = len(A), len(A[0])\n",
    "    assert len(B) == n, f\"shape mismatch {mat_shape(A)} @ {mat_shape(B)}\"\n",
    "    p = len(B[0])\n",
    "    out = [[0.0]*p for _ in range(m)]\n",
    "    for i in range(m):\n",
    "        for k in range(n):\n",
    "            aik = A[i][k]\n",
    "            for j in range(p):\n",
    "                out[i][j] += aik * B[k][j]\n",
    "    return out\n",
    "\n",
    "def transpose(A):\n",
    "    '''Return the transpose of matrix A (list of rows -> list of columns).'''\n",
    "    return [list(row) for row in zip(*A)]\n",
    "\n",
    "def add(A, B):\n",
    "    '''Elementwise add for vectors or matrices (nested lists).'''\n",
    "    if isinstance(A[0], list):\n",
    "        return [[a+b for a,b in zip(ra, rb)] for ra, rb in zip(A,B)]\n",
    "    return [a+b for a,b in zip(A,B)]\n",
    "\n",
    "def sub(A, B):\n",
    "    '''Elementwise subtract for vectors or matrices (nested lists).'''\n",
    "    if isinstance(A[0], list):\n",
    "        return [[a-b for a,b in zip(ra, rb)] for ra, rb in zip(A,B)]\n",
    "    return [a-b for a,b in zip(A,B)]\n",
    "\n",
    "def scalar_mul(A, s):\n",
    "    '''Multiply every element of vector or matrix A by scalar s.'''\n",
    "    if isinstance(A[0], list):\n",
    "        return [[a*s for a in row] for row in A]\n",
    "    return [a*s for a in A]\n",
    "\n",
    "def dot(u, v):\n",
    "    '''Dot product of two vectors.'''\n",
    "    return sum(ui*vi for ui,vi in zip(u,v))\n",
    "\n",
    "def norm(v):\n",
    "    '''Euclidean (L2) norm of a vector, with tiny floor for stability.'''\n",
    "    return math.sqrt(max(1e-12, dot(v,v)))\n",
    "\n",
    "def cosine_similarity(u, v):\n",
    "    '''Cosine similarity of two vectors (angle-based similarity).'''\n",
    "    return dot(u, v) / (norm(u) * norm(v))\n",
    "\n",
    "def argmax(xs):\n",
    "    '''Return the index of the largest value in a list.'''\n",
    "    return max(range(len(xs)), key=lambda i: xs[i])\n",
    "\n",
    "def one_hot(idx, n):\n",
    "    '''One-hot vector of length n with 1 at idx.'''\n",
    "    v = [0.0]*n\n",
    "    v[idx] = 1.0\n",
    "    return v\n",
    "\n",
    "def softmax(xs):\n",
    "    '''\n",
    "    Convert a list of real numbers to a probability distribution.\n",
    "    We subtract max(xs) for numerical stability (avoids overflow).\n",
    "    '''\n",
    "    m = max(xs)\n",
    "    exps = [math.exp(x - m) for x in xs]\n",
    "    Z = sum(exps)\n",
    "    return [e / Z for e in exps]\n",
    "\n",
    "def softmax_rows(M):\n",
    "    '''Row-wise softmax for a 2D list (matrix).'''\n",
    "    return [softmax(row) for row in M]\n",
    "\n",
    "def cross_entropy(p, q):\n",
    "    '''Cross-entropy between two probability vectors p and q.'''\n",
    "    eps = 1e-12\n",
    "    return -sum(pi * math.log(max(qi, eps)) for pi, qi in zip(p,q))\n",
    "\n",
    "def cross_entropy_rows(P, Q):\n",
    "    '''Average cross-entropy across batches (row-wise).'''\n",
    "    return sum(cross_entropy(p, q) for p, q in zip(P, Q)) / len(P)\n",
    "\n",
    "def gelu(x):\n",
    "    '''GELU activation (approximation).'''\n",
    "    return 0.5 * x * (1.0 + math.tanh(math.sqrt(2.0/math.pi)*(x + 0.044715*(x**3))))\n",
    "\n",
    "def relu(x):\n",
    "    '''ReLU activation.'''\n",
    "    return x if x > 0 else 0.0\n",
    "\n",
    "def relu_vec(v):\n",
    "    '''Apply ReLU to every element of a vector.'''\n",
    "    return [relu(x) for x in v]\n",
    "\n",
    "def apply_rowwise(A, fn):\n",
    "    '''Apply a scalar function elementwise to each row of a matrix.'''\n",
    "    return [[fn(x) for x in row] for row in A]\n",
    "\n",
    "def randn_matrix(m, n, std=0.02):\n",
    "    '''Gaussian random matrix with mean 0 and standard deviation std.'''\n",
    "    return [[random.gauss(0.0, std) for _ in range(n)] for _ in range(m)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e76556",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# 🧩 LESSON 1 — Linear Algebra Foundations\n",
    "\n",
    "**Concepts:** vectors, dot products, norms, angles, matrix multiplication, transpose, cosine similarity.  \n",
    "**Why it matters:** Every neural net layer is basically `y = xW + b`. Attention uses `Q @ K^T`.  \n",
    "Understanding these basics makes the rest feel natural.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609bd3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "banner(\"LESSON 1: Linear Algebra Foundations\")\n",
    "\n",
    "# Example vectors\n",
    "u = [1, 2, 3]\n",
    "v = [2, 1, 0]\n",
    "\n",
    "print(\"u =\", u)\n",
    "print(\"v =\", v)\n",
    "print(\"dot(u, v) =\", dot(u, v))\n",
    "print(\"norm(u)   =\", round(norm(u), 4))\n",
    "print(\"cosine_similarity(u, v) =\", round(cosine_similarity(u, v), 4))\n",
    "\n",
    "# Example matrices\n",
    "A = [\n",
    "    [1, 2],\n",
    "    [3, 4],\n",
    "    [5, 6],\n",
    "]  # shape (3x2)\n",
    "\n",
    "B = [\n",
    "    [7, 8, 9],\n",
    "    [10, 11, 12],\n",
    "]  # shape (2x3)\n",
    "\n",
    "print(\"\\nMatrix A (3x2):\", A)\n",
    "print(\"Matrix B (2x3):\", B)\n",
    "\n",
    "C = matmul(A, B)  # (3x3)\n",
    "print(\"\\nC = A @ B (3x3):\")\n",
    "for row in C:\n",
    "    print([round(x, 3) for x in row])\n",
    "\n",
    "Ct = transpose(C)\n",
    "print(\"\\ntranspose(C):\")\n",
    "for row in Ct:\n",
    "    print([round(x, 3) for x in row])\n",
    "\n",
    "print(\"\\nTry it:\")\n",
    "print(\"1) Change entries of u and v and observe cosine similarity.\")\n",
    "print(\"2) Create your own matrices and check shape compatibility for matmul.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c088ec4",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# 🎲 LESSON 2 — Probability & Softmax\n",
    "\n",
    "**Concepts:** turning scores (logits) into probabilities (softmax), comparing distributions (cross‑entropy).  \n",
    "**Why it matters:** Classifiers output logits; training minimizes cross‑entropy with the target distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e295316a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "banner(\"LESSON 2: Probability & Softmax\")\n",
    "\n",
    "logits = [2.0, 1.0, 0.1]\n",
    "probs = softmax(logits)\n",
    "target = one_hot(0, 3)  # pretend the correct class is index 0\n",
    "\n",
    "print(\"logits:\", logits)\n",
    "print(\"softmax(logits):\", [round(p, 4) for p in probs])\n",
    "print(\"target one-hot:\", target)\n",
    "print(\"cross_entropy(target, probs):\", round(cross_entropy(target, probs), 6))\n",
    "\n",
    "# Row-wise demo (mini-batch of two examples)\n",
    "batch_logits = [\n",
    "    [1.0, -1.0, 0.0],\n",
    "    [0.1, 0.2, 0.3],\n",
    "]\n",
    "batch_probs = softmax_rows(batch_logits)\n",
    "batch_targets = [one_hot(2,3), one_hot(1,3)]\n",
    "\n",
    "print(\"\\nBatch softmax:\")\n",
    "for row in batch_probs:\n",
    "    print([round(x, 4) for x in row])\n",
    "\n",
    "print(\"Batch CE:\", round(cross_entropy_rows(batch_targets, batch_probs), 6))\n",
    "\n",
    "print(\"\\nTry it:\")\n",
    "print(\"1) Make the correct class logit larger and see CE go down.\")\n",
    "print(\"2) Add a huge number to all logits—does softmax change? Why not? (Hint: invariance)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a197c7",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# 📉 LESSON 3 — Calculus & Optimization (Backprop Basics)\n",
    "\n",
    "**Concepts:** gradient, numerical vs. analytical derivatives, gradient descent.  \n",
    "**Why it matters:** Training = minimizing a loss by following the gradient downhill.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289e507b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "banner(\"LESSON 3: Calculus & Optimization\")\n",
    "\n",
    "def gradient_descent_1d(f, df, x0, lr=0.1, steps=25, verbose=True):\n",
    "    x = x0\n",
    "    history = []\n",
    "    for t in range(steps):\n",
    "        grad = df(x)\n",
    "        x = x - lr * grad\n",
    "        fx = f(x)\n",
    "        history.append((t+1, x, fx))\n",
    "        if verbose:\n",
    "            print(f\"step {t+1:02d}: x={x:.5f}, f(x)={fx:.8f}, grad={grad:.5f}\")\n",
    "    return x, history\n",
    "\n",
    "def numeric_grad(f, x, eps=1e-5):\n",
    "    return (f(x+eps) - f(x-eps)) / (2*eps)\n",
    "\n",
    "# Minimize f(x) = (x - 3)^2\n",
    "f  = lambda x: (x-3)**2\n",
    "df = lambda x: 2*(x-3)\n",
    "\n",
    "x_star, hist = gradient_descent_1d(f, df, x0=0.0, lr=0.2, steps=15, verbose=True)\n",
    "print(\"\\nargmin x* ≈\", round(x_star, 6), \"f(x*) =\", round(f(x_star), 12))\n",
    "print(\"numeric_grad at x=3.0 ->\", numeric_grad(f, 3.0))\n",
    "\n",
    "print(\"\\nTry it:\")\n",
    "print(\"1) Change lr (learning rate) to 1.5 and watch it diverge.\")\n",
    "print(\"2) Change steps to 50 and watch convergence slow/fast.\")\n",
    "print(\"3) Replace f with a non-convex function (e.g., sin(x) + 0.1x^2).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21ed0bb",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# 🧪 LESSON 4 — Tiny Neural Net From Scratch (XOR)\n",
    "\n",
    "**Concepts:** linear layers, activations, softmax + cross‑entropy, *manual* backprop.  \n",
    "**Why it matters:** Before Transformers, all deep nets are compositions of linear maps and nonlinearities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60aaa2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "banner(\"LESSON 4: Tiny NN From Scratch (XOR)\")\n",
    "\n",
    "def init_layer(in_dim, out_dim, std=0.5):\n",
    "    W = randn_matrix(out_dim, in_dim, std=std)  # rows=out_dim\n",
    "    b = [[0.0] for _ in range(out_dim)]\n",
    "    return W, b\n",
    "\n",
    "def lin_forward(X, W, b):\n",
    "    # X: (N x D), W: (O x D), b: (O x 1) -> (N x O)\n",
    "    return add(matmul(X, transpose(W)), transpose(b))\n",
    "\n",
    "def relu_forward(X):\n",
    "    return apply_rowwise(X, relu)\n",
    "\n",
    "def softmax_forward(X):\n",
    "    return softmax_rows(X)\n",
    "\n",
    "def xor_dataset():\n",
    "    X = [\n",
    "        [0.0, 0.0],\n",
    "        [0.0, 1.0],\n",
    "        [1.0, 0.0],\n",
    "        [1.0, 1.0],\n",
    "    ]\n",
    "    Y = [\n",
    "        [1.0, 0.0],  # class 0\n",
    "        [0.0, 1.0],  # class 1\n",
    "        [0.0, 1.0],\n",
    "        [1.0, 0.0],\n",
    "    ]\n",
    "    return X, Y\n",
    "\n",
    "def train_xor(epochs=2000, lr=0.1, hidden=4, verbose=True):\n",
    "    X, T = xor_dataset()      # X: (4x2), T: (4x2 one-hot)\n",
    "    D_in, D_h, D_out = 2, hidden, 2\n",
    "\n",
    "    # Init layers\n",
    "    W1, b1 = init_layer(D_in, D_h, std=0.8)  # (h x 2), (h x 1)\n",
    "    W2, b2 = init_layer(D_h, D_out, std=0.8) # (2 x h), (2 x 1)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Forward\n",
    "        Z1 = lin_forward(X, W1, b1)        # (4 x h)\n",
    "        A1 = relu_forward(Z1)\n",
    "        Z2 = lin_forward(A1, W2, b2)       # (4 x 2)\n",
    "        Y  = softmax_forward(Z2)           # (4 x 2)\n",
    "\n",
    "        # Loss (Cross-Entropy)\n",
    "        loss = cross_entropy_rows(T, Y)\n",
    "\n",
    "        # Backprop (manual, small network)\n",
    "        # dL/dZ2 = Y - T   (from softmax + CE)\n",
    "        dZ2 = [[(y - t) for y,t in zip(yrow, trow)] for yrow, trow in zip(Y, T)]  # (4 x 2)\n",
    "\n",
    "        # Grad W2 = A1^T @ dZ2\n",
    "        dW2 = matmul(transpose(A1), dZ2)   # (h x 2)\n",
    "        db2 = [[sum(col)] for col in transpose(dZ2)]  # (2 x 1)\n",
    "\n",
    "        # Back to layer1: dA1 = dZ2 @ W2\n",
    "        dA1 = matmul(dZ2, W2)              # (4 x h)\n",
    "        # ReLU backprop: dZ1 = dA1 * (Z1 > 0)\n",
    "        dZ1 = []\n",
    "        for i in range(len(Z1)):\n",
    "            dZ1.append([dA1[i][j] * (1.0 if Z1[i][j] > 0 else 0.0) for j in range(len(Z1[0]))])\n",
    "\n",
    "        # Grad W1 = X^T @ dZ1 (then transpose to match shape)\n",
    "        dW1 = transpose(matmul(transpose(X), dZ1))    # (h x 2)\n",
    "        db1 = [[sum(col)] for col in transpose(dZ1)]  # (h x 1)\n",
    "\n",
    "        # Gradient step\n",
    "        W2 = sub(W2, scalar_mul(dW2, lr))\n",
    "        b2 = sub(b2, scalar_mul(db2, lr))\n",
    "        W1 = sub(W1, scalar_mul(dW1, lr))\n",
    "        b1 = sub(b1, scalar_mul(db1, lr))\n",
    "\n",
    "        if verbose and (epoch+1) % 500 == 0:\n",
    "            preds = [argmax(y) for y in Y]\n",
    "            acc = sum(int(p == argmax(t)) for p,t in zip(preds, T)) / len(T)\n",
    "            print(f\"epoch {epoch+1:4d} | loss {loss:.4f} | acc {acc:.2f}\")\n",
    "\n",
    "    # Final sanity check\n",
    "    print(\"\\nPredictions after training: (probabilities then class)\")\n",
    "    X, T = xor_dataset()\n",
    "    # Run one more forward for printing\n",
    "    Z1 = lin_forward(X, W1, b1); A1 = relu_forward(Z1)\n",
    "    Z2 = lin_forward(A1, W2, b2); Y = softmax_forward(Z2)\n",
    "    for x, y, t in zip(X, Y, T):\n",
    "        print(f\"X={x} -> P={ [round(v,3) for v in y] } -> pred={argmax(y)} | true={argmax(t)}\")\n",
    "\n",
    "    return (W1, b1, W2, b2)\n",
    "\n",
    "# Train the tiny net\n",
    "_ = train_xor(epochs=2000, lr=0.1, hidden=4, verbose=True)\n",
    "\n",
    "print(\"\\nTry it:\")\n",
    "print(\"1) Change hidden size to 2 or 8. Does it learn faster/slower?\")\n",
    "print(\"2) Replace ReLU with GELU and compare.\")\n",
    "print(\"3) Try MSE loss instead of cross-entropy and observe differences.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e805e5ad",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "# 🔭 LESSON 5 — Scaled Dot‑Product Attention\n",
    "\n",
    "**Concepts:** projections (Q, K, V), similarity via dot products, scaling by √d, softmax over scores, causal masks.  \n",
    "**Why it matters:** This operation is the *heart* of GPT‑style Transformers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0b908f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "banner(\"LESSON 5: Scaled Dot-Product Attention\")\n",
    "\n",
    "def causal_mask(n):\n",
    "    '''Lower-triangular mask (n x n) with 0 above diagonal, 1 elsewhere.'''\n",
    "    return [[1.0 if j <= i else 0.0 for j in range(n)] for i in range(n)]\n",
    "\n",
    "def masked_fill(M, mask, fill_value=-1e9):\n",
    "    '''Like PyTorch masked_fill, for nested lists.'''\n",
    "    out = []\n",
    "    for i in range(len(M)):\n",
    "        row = []\n",
    "        for j in range(len(M[0])):\n",
    "            row.append(M[i][j] if mask[i][j] > 0.5 else fill_value)\n",
    "        out.append(row)\n",
    "    return out\n",
    "\n",
    "def attention(Q, K, V, causal=False):\n",
    "    '''\n",
    "    Q: (T x d), K: (T x d), V: (T x dv)\n",
    "    returns: (T x dv), (T x T) weights, (T x T) scores\n",
    "    '''\n",
    "    d = len(Q[0])\n",
    "    scores = matmul(Q, transpose(K))               # (T x T)\n",
    "    scores = [[s / math.sqrt(d) for s in row] for row in scores]\n",
    "    if causal:\n",
    "        M = causal_mask(len(scores))\n",
    "        scores = masked_fill(scores, M, fill_value=-1e9)\n",
    "    weights = softmax_rows(scores)                 # (T x T)\n",
    "    out = matmul(weights, V)                       # (T x dv)\n",
    "    return out, weights, scores\n",
    "\n",
    "def linear_project(X, W):\n",
    "    # X: (T x d_in), W: (d_in x d_out)  -> (T x d_out)\n",
    "    return matmul(X, W)\n",
    "\n",
    "def demo_attention(T=4, d=6, dv=6, causal=True):\n",
    "    # Toy token embeddings (sequence length T, dim d)\n",
    "    X = [[random.gauss(0.0, 1.0) for _ in range(d)] for _ in range(T)]\n",
    "\n",
    "    # Learnable projections (random init for demo)\n",
    "    Wq = randn_matrix(d, d, std=0.4)\n",
    "    Wk = randn_matrix(d, d, std=0.4)\n",
    "    Wv = randn_matrix(d, dv, std=0.4)\n",
    "\n",
    "    Q = linear_project(X, Wq)   # (T x d)\n",
    "    K = linear_project(X, Wk)   # (T x d)\n",
    "    V = linear_project(X, Wv)   # (T x dv)\n",
    "\n",
    "    Y, W, S = attention(Q, K, V, causal=causal)\n",
    "\n",
    "    print(f\"Sequence length T={T}, dim d={d}, value dim dv={dv}, causal={causal}\")\n",
    "    print(\"\\nFirst token embedding X[0]:\")\n",
    "    print([round(x,3) for x in X[0]])\n",
    "\n",
    "    print(\"\\nScaled scores S = Q @ K^T / sqrt(d):\")\n",
    "    for row in S:\n",
    "        print([round(s, 3) for s in row])\n",
    "\n",
    "    print(\"\\nAttention weights (rows sum to 1):\")\n",
    "    for row in W:\n",
    "        print([round(w, 3) for w in row])\n",
    "\n",
    "    print(\"\\nOutput (context) vectors Y (one per token):\")\n",
    "    for row in Y:\n",
    "        print([round(y, 3) for y in row])\n",
    "\n",
    "demo_attention(T=4, d=6, dv=6, causal=True)\n",
    "\n",
    "print(\"\\nTry it:\")\n",
    "print(\"1) Set causal=False and observe attention to 'future' tokens.\")\n",
    "print(\"2) Increase d to 32 and note how scaling affects softmax sharpness.\")\n",
    "print(\"3) Make two tokens identical in X and watch their mutual attention grow.\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
